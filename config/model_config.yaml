# LLM Model Configuration
# Version: 1.0.0

models:
  gpt4:
    provider: openai
    model_name: gpt-4-turbo-preview
    temperature: 0.7
    max_tokens: 2000
  
  claude:
    provider: anthropic
    model_name: claude-3-opus-20240229
    temperature: 0.7
    max_tokens: 4000

rate_limits:
  openai:
    requests_per_minute: 20
    tokens_per_minute: 90000
  
  anthropic:
    requests_per_minute: 15
    tokens_per_minute: 100000

retry:
  max_attempts: 3
  backoff_factor: 2
  backoff_max: 60

cache:
  enabled: true
  ttl_seconds: 3600
  max_size_mb: 500
  directory: data/cache

logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: logs/app.log
  max_bytes: 10485760
  backup_count: 5
